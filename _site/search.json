[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mark Betnel",
    "section": "",
    "text": "Education\n\nBS Physics (Harvey Mudd ’98)\nMA Philosophy (San Francisco State ’01)\nMS Physics (University of Rhode Island ’05)\nPhD Physics (Boston University ’11)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Not that anyone’s asking me to, but I refuse to work on weapons or surveillance. That committment led me to study physics and literature as an undergraduate, then to teach high school and go to graduate school in philosophy where I wrote about moral agency and propaganda. Then when I went back to grad school again, in physics, I chose research projects that were funded outside the defense ecosystem, and focused on my teaching practice and my young family (spouse and newborn).\nAfter my defense, I got a teaching job at Johnson & Wales University in Providence, where I had a very busy two years that included the birth of our second child and teaching eleven different preps in 6 terms of teaching, out of two different departments.\nMy wife and I made a difficult choice to leave Rhode Island to move to Seattle so our kids could grow up near their grandparents and cousins, and I took a job at a private high school in Seattle. I’m now in my tenth year in that job, and have taught courses in three departments, created a number of new classes, and helped to develop and launch a new department to offer courses in computational thinking. I am most proud of increasing enrollment and diversity in our senior physics elective, and of the course I created on the technical and ethical implications of machine learning.\nOutside of my main job, I also teach a night course at a local community college, coach youth soccer teams for my kids, am an active member of a Quaker meeting, and I work on writing/education about the uses and implications of technology and issues in education."
  },
  {
    "objectID": "nullresult/about.html",
    "href": "nullresult/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "nullresult/index.html",
    "href": "nullresult/index.html",
    "title": "nullresult",
    "section": "",
    "text": "news\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 26, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "nullresult/posts/post-with-code/index.html",
    "href": "nullresult/posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "nullresult/posts/welcome/index.html",
    "href": "nullresult/posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Mark Betnel",
    "section": "Education",
    "text": "Education\n\nBS Physics (Harevey Mudd)\nMA Philosophy (San Francisco State)\nMS Physics (University of Rhode Island)\nPhD Physics (Boston University)"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "null result",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngpt\n\n\nai\n\n\nexpertise\n\n\nethics\n\n\npolicy\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2023\n\n\nMark Betnel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nai\n\n\nllm\n\n\ngpt\n\n\nfuture\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2023\n\n\nMark Betnel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nai\n\n\nllm\n\n\ngpt\n\n\nfuture\n\n\nsports\n\n\ntraining\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2023\n\n\nMark Betnel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nct\n\n\nunits\n\n\nphysics\n\n\njs\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2022\n\n\nMark Betnel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nct\n\n\nlabelling\n\n\nstudents\n\n\neducation\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2022\n\n\nMark Betnel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nct\n\n\nsngn\n\n\nphysics\n\n\nmodelling\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2021\n\n\nMark Betnel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncircle\n\n\ngeometry\n\n\ntrigonometry\n\n\nalgebra\n\n\nproblems\n\n\nconstruction\n\n\nsafety\n\n\nletting go\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2015\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/labelling/index.html",
    "href": "blog/posts/labelling/index.html",
    "title": "Labelling Demo",
    "section": "",
    "text": "This class, being an elective, means at my school that I can’t assign (much) homework. It having no pre-requisites means I can count on no background knowledge about computing. This is by design, as I want everyone in the school to take the class. I’m getting about 80 kids per year in it, so maybe 50% of the school will have taken it at some point – I feel good about that. But that still means it’s challenging to teach.\nWhat I want is for them to come away with an informed curiousity about the technology and its implications, so they can ask good questions about it and make good decisions about it in their own lives and for society. That’s a good goal, but it’s kind of nebulous for defining the performances I want them to be capable of during the class. What ends up happening is a bunch of reading, playing with little toy examples, and discussions that are a little one-sided – I have a lot to learn about how to teach a class like this.\nSo one of the concret-ish things I want from them is to understand the workflow of supervised learning and deployment: data gathering, curation, and labelling; model training; model testing; deployment; maintenance; end-user usage; iteration. With each of those steps I want them to know a little technical detail and to think about some of the fraught questions that can arise (and that they should ask!)\nFor example: Why do we need an ML tool for this task? Is it plausible that there is signal in the available data to power the ML tool? If the desired tool really works exactly as designed, does it pass the “Is it creepy?” test? What are the broader implications of success or failure with the project? Where does the data come from? How does it get labelled? How is privacy of data subjects protected? Is informed consent obtained? What are the training costs and issues? What does it mean to overfit vs underfit? How does one test the results? What are the error rates? Is there more than one kind of error? Do the different kinds of errors have costs (for the user and the target)? Do the errors (and successes) fall on population subgroups equitably? How is the model used in deployment? How is the performance monitored once deployed? Are there plans to maintain the system and to bring it down if results are not as expected? What would the next generation tool look like? Will it be more data, better labelling, more training compute, more sophisticated testing, better deployment infrastructure, better user education…\nThat’s a lot of questions. I asked the students this week to make a flowchart of what they thought the steps were – as a kind of formative assessment for me of what ideas had landed and which hadn’t. One thing I noticed is that a lot of them went straight from “Identify problem” to “Train computer to solve problem” without any kind of “acquire labelled data” step – that’s definitely a step that I thought we’d spent enough time discussing, so I was thinking about finding a demo of how labelling works – something like the workflow of a Mechanical Turk-er, and then present some stats on the quality of the labelling so they can think about issues of reliability of the labels, the effect of the labels on the training process, and how well the data set covers the relevant universe of things. I couldn’t find anything like that.\nSo I’ve been trying to make it. Progress reports to follow, but it turns out that I’ve never done server side programming before, and it’s a fine rabbit hole."
  },
  {
    "objectID": "blog/posts/post-with-code/index.html",
    "href": "blog/posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "blog/posts/skiing_magee/skiing_magee.html",
    "href": "blog/posts/skiing_magee/skiing_magee.html",
    "title": "The Skiing Magee",
    "section": "",
    "text": "Mr. Magee decides one day that he wants to go skiing, but doesn’t know how, so he packs up his dog and climbs to the top of a local hill and just starts skiing down. It’s going great – much better than he has a right to expect on his first try, when he sees a moose in his path ahead. The moose is completely unaware of Mr. Magee, it’s fixated on a tasty looking tree on the other side of a chasm in the snow, and wondering how to get across.\nMr. Magee meanwhile is shouting for the moose to move because he doesn’t know how to steer, when he suddenly leans back and slides on his butt, under the moose, catching the tips of the skis on a log. He then flips entirely upside down, landing with the back of the skis on the other side of the chasm, the front tips lodged under the log on the uphill side of the chasm, and he, poor Mr. Magee, is hanging upside down from the skis!\nThen the moose turns around.\nWondering where Mr. Magee had gone, the moose does see a bridge, built just for him, across the chasm, and begins to walk across on top of the skis.\nThe skis bend, but do not break, when the moose looks down to see Mr. Magee starting up at him in terror. So of course the moose jumps in fright, pushing Mr. Magee down some more, but landing safely on the downhill side where he can go eat that tasty tree.\nMr. Magee on the other hand, is sproinged up in the air, flips right side up, and lands safely on his skis on the uphill side of the chasm!!\nSo of course Mr. Magee goes home and puts his skis to a better use for a while.\nWith this story I’m seeing a physics lesson, centered around rotational motion, forces, and elastic stretches.\nSome possible questions:\n\nHow much does the snow compress under the tip and back of the skis when Mr. Magee flips over? How much more does it compress when the moose steps on him?\nCan we infer the elastic properties of the skis from how much they stretch when Mr. Magee is hanging upside down?\nWhat are all the interactions and forces while the moose is jumping off the skis? How might this cause Mr. Magee to get flipped right side up?\nIs moose going forward and Magee going backward an example of “equal and opposite”?\nHow strong must the log-fulcrum be to cause Magee to flip upside down? How fast must the rotational motion be?\n\nMy possible questions from this story are a lot more “physics teacher-y” than some of the other stories I’m thinking about, but they’re a lot of fun and might lead to some cool modelling opportunities.\nThe Skiing Magee is a part of a planned series of posts analyzing mathematical, computational, and scientific themes in children’s stories as part of possibly writing a book playing some fun games with these stories."
  },
  {
    "objectID": "blog/posts/welcome/index.html",
    "href": "blog/posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blog/posts/four_circles/index.html",
    "href": "blog/posts/four_circles/index.html",
    "title": "Four Circles",
    "section": "",
    "text": "Construct a circle that is mutually tangent to three other circles, each of which is tangent to each of the others. The circles have radii of 3, 2, and 1. What is the radius of the fourth circle? And how do you locate its center?\n.. raw:: html\n  ..\nOpen in a separate tab. <http://web.geogebra.org/app/?id=952253>_\nIt was an interesting experience working on the problem with other teachers in the room. It was intimidating.\nThe problem poser and a third teacher who had seen the problem before but didn’t remember the solution were looking over my shoulder while I was sketching and thinking.\nSo intimidating\nI found myself falling prey to all the mental traps I see my students fall into.\n“It’s taking too long. I must have missed something.”\n“Are they going to respect me if I can’t figure it out?”\n“This approach looks promising, but it doesn’t seem elegant enough to justify the problem-poser’s satisfaction.”\n“Maybe I should make a new drawing. This one isn’t very neat.”\nand the third teacher, who half-remembered having done the problem before, admitted that he was falling prey to the trap of trying to remember the solution, rather than thinking afresh about it (one of the more common traps my students fall into).\nSo I’m taking away from this a reminder of how important it is to make the space safe for a student to experiment, and how easy it is to screw it up by asking the wrong question at the wrong time in the wrong way. Or even just by hovering a little too soon.\nI’m reminded of Feynman’s anecdote about the high school students he had working for him at Los Alamos near the end of the Manhattan Project. They had come up with a clever way to speed up their calculations, but they were still working out the bugs when he walked in the room, and, embarrased and worried that he would screw them up, they shouted at him to get out.\nI think Feynman is a problematic figure in science, but we could all think about following this one example at least, if we really want our students to feel safe: without a word, he closed the door and walked away."
  },
  {
    "objectID": "blog/about.html",
    "href": "blog/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog/posts/unit-tutor/unit_tutor.html",
    "href": "blog/posts/unit-tutor/unit_tutor.html",
    "title": "Unit Tutor",
    "section": "",
    "text": "My hope was that the ease of maniuplating quantities and seeing the results of a calculation would help students better understand things like why multipying “30 meters per second” by “1 minute per 60 seconds” won’t tell you how many meters in one minute.\nLast spring I finally started to feel like I might be able to create that site, and I spent a significant amount of my release time in May working on a prototype, which is now live at https://itmeson.github.io/unit-tutor/.\nHere I’ve made an example (not live) of a calculation where I was estimating how much water is lost in a month from a faucet dripping once per second.\n\n\n\n\n\nAn example of a computation/unit conversion\n\n\nThere are still a lot of issues with the prototype that a “real” engineer might be able to solve quickly, some of which are listed on the github readme for the project, but I feel like I’m ready for a few people to try it out to give me some feedback on more issues and what’s worth putting the effort into and what’s “okay” as-is.\nIssues can be reported using github issue tracking, or in this google form which is also linked on the project page.\nWhat I hope to do in the near-term is build a couple of tutorial-style pages that walk users/students through example unit-conversion problems, provide an easy way to make worksheet-style pages that have pre-populated conversion factors and word problems that students can parse to turn into calculations, and an open-calculator mode where the conversion factor tab brings up a drop-down menu of known conversion factors, the quantities tab allows free entry of measured values, and the calculate results can be re-used in more involved multi-step calculations."
  },
  {
    "objectID": "blog/posts/cave/cave.html",
    "href": "blog/posts/cave/cave.html",
    "title": "Caves",
    "section": "",
    "text": "If I map the allegory onto language, then my thoughts are the “real” objects, and the words I produce are the light that carries the signal about those thoughts to others, who, on hearing the shadow-words, construct theories about the real thoughts that produced them. When they respond with words of their own, casting shadow puppets back to me, I form theories about the thoughts that produced them, and thus refine my own thoughts.\nOne use of large language models (LLMs) is to facilitate communication, by taking over some of the work of composing the text that I might send to someone else. This is introducing another layer of translation into my system of shadow words – I have a thought I wish to communicate (or a topic I wish to communicate about), so I compose a prompt for the LLM. The LLM produces text that matches the prompt. If I am careful, I will edit the result – thus getting one layer of feedback about my thoughts, causing me to refine those thoughts. The produced message will not be precisely what I intended (it never is and never has been), but I will send it anyway as it’s close enough. The receiver won’t read the message, it will be too long, and anyway, they will know that it was produced by an LLM not by me, so there’s no insult in feeding the message into their own LLM. The receiver’s LLM will interpret the result and possibly summarize it for the receiver, who will form theories about the mind that produced the text before responding. Or perhaps the receiving LLM will skip that step and just compose the reply. Either way, I will receive a reply that is mediated by one or more translations through another LLM, finally causing me to update my own thoughts based on the feedback.\nAre there instances where these extra layers will enable communication with greater fidelity? Probably. Will there be instances where the layers introduce more noise? Or steer my thoughts? Certainly. And if I don’t control the intermediary LLMs, how can I be sure that the steering isn’t intended to manipulate me, the way every other algorithmic communication system has been dual purposed for manipulation?\nIn the allegory the philosopher is supposed to fight to get free of the cave."
  },
  {
    "objectID": "blog/posts/sports/sports.html",
    "href": "blog/posts/sports/sports.html",
    "title": "Sports Metaphors",
    "section": "",
    "text": "And how many of those kids ever get paid to play that sport? None of them? To a first approximation.\nSo why do we put kids into youth sports? Sure there’s a few people who are convinced that their kid is going to be the one, and a few kids who are convinced by the dream. And a few of them actually do make it. But the reason we do it must be about something other than the future financial payoff.\nThe reasons we’ll say tend to be about fitness, teamwork, leadership, tactical thinking, and culture. Of course, if it was just about fitness, we’d probably be doing a lot more gymnastics and weights – individual sports. But team sports are more fun. And it’s easier to share them with spectators.\nAnd I think all of those are great reasons to get kids into sports, even if none of them become professionals.\nSo how many kids learn math every year? All of them? To a first approximation.\nAnd how many of those kids ever get paid to do math? Well, it’s not none of them this time. In fact, I think most people do use some mathematical thinking in their work. But the number who get paid to really do math? Not very many. So why do we put kids into math?\nHow many kids learn intellectual skills every year? All of them?\nAnd how many will ever “turn pro” as an intellectual? Arguably, all of them, if you have a broad understanding of what intellectual skills are and how they are used.\nWhat about in the near future, when ubiquitous AGI does most of the intellectual work that needs doing? Will there be a time when a small group of elite professional intellectuals are the only ones who’ve survived the gauntlet of the minor leagues of high school, college, and graduate school to get to turn pro? Will they have an NFL career (not-for-long) while they are still able to keep up with the best of the best, before they inevitably slow down and have to settle into a long retirement telling stories of their glory days developing new theorems?\nWill it still be worth teaching those intellectual skills to most or all students, even though only a tiny fraction will ever need to use them?"
  },
  {
    "objectID": "blog/posts/expertise/expertise.html",
    "href": "blog/posts/expertise/expertise.html",
    "title": "GPT Is Not All You Need",
    "section": "",
    "text": "There’s obviously a tension. Anti-maskers and anti-vaxxers express a similar-sounding view, “I’m just doing my own research, and I don’t think these vaccines are safe,” they say, arguing that they should get to decide for themselves. When decisions truly only affect the individual, I’m content to leave it as an individual decision. But public health affects everyone, so there has to be room for experts to inform the public about the technical issues, and room for political processes to decide how to use that information in making recommendations and rules. Technical expertise informs technical questions, and the answer to those technical questions inform critical reflection and judgement. Importantly, it’s not just the opinions of vaccine developers which count for the technical questions – there’s an FDA whose purpose is to insure that the vaccines are safe AND effective.\n\n\n\n(Tweet from Yann LeCun.)\n\n\nYann LeCun thinks that builders of AI are the only ones whose opinion really counts. They aren’t afraid of AGI, so the rest of us shouldn’t be either.\nFirst, that’s not true – there are a number of people who work at the cutting edge of AI research and development who are worried about the potential harms of AGI (and at least as importantly, who worry about the actual current harms from irresponsible deployment of AI tools). But I think that’s a red herring anyway, as the relevant expertise here isn’t technical.\nThe only technical insight one needs is that AI systems are developed in a way to maximize their “score” on measurements of how well their outputs reflect the programmers’ goals. If the score measurement perfectly reflects the engineered goals (and those goals are good ones), then there isn’t a problem. But the score measurement never perfectly reflects the engineered goals. I’ve tried to measure student learning in many different ways in my career, but I’ve never managed to find a way to do that that prevented students from “gaming” the system. There always seems to be a way to get a good score without actually learning, and some students always seem to find it.\nMachine learning systems are also quite creative at gaming their scoring systems, allowing them to get good scores without actually doing the thing we wanted them to do. Some examples of this are just funny. Some are pretty disturbing. The point, for those of us who are worried but aren’t cutting edge AI researchers like Yan LeCun, is that engineered products are supposed to satisfy relevant safety guidelines. Pharmaceutical companies have to demonstrate that their products are safe and effective before they can be used. But there isn’t an FDA for algorithms (yet), and if the only opinions that matter are those of the developers, we’re stuck trusting folks who are motivated more by the potential profit than by the relevant ethical questions, and even then those experts don’t really know how safe and effective their products are.\nSo absent any independent assurances, my questions and expertise are just as relevant as LeCun’s. I think that attacking the “information space” with a misinformation bomb is a problem. Deploying automated decision making tools without taking legal responsibility for the outcomes is a problem. Moving fast and breaking the education system with a tool that subverts our entire assessment framework is a problem. There’s of course many more – all of which are going to require some solutions or mitigations or just deciding not to do that.\nIt would be great if the leaders in developing these tools also seemed to be interested in collaborating with the rest of us to figure out which tools might be useful, which need safeguards, and which shouldn’t be used at all. In figuring out what we ought to do, GPT is definitely not all you need."
  },
  {
    "objectID": "blog/posts/GPT Is Not All You Need/GPT Is Not All You Need.html",
    "href": "blog/posts/GPT Is Not All You Need/GPT Is Not All You Need.html",
    "title": "GPT Is Not All You Need",
    "section": "",
    "text": "–\nOne of the pillars of my life mission is that humans don’t have to earn the right to think critically. The sneering “You’re not an expert, so you’ll just have to let your betters tell you what to think” crowd has been a consistent frustration for decades. To me, one of the goals of education is freedom from the “tyranny of experts”, not to replace expertise, but to learn enough to be able to ask good questions to inform critical judgements.\nThere’s obviously a tension. Anti-maskers and anti-vaxxers express a similar-sounding view, “I’m just doing my own research, and I don’t think these vaccines are safe,” they say, arguing that they should get to decide for themselves. When decisions truly only affect the individual, I’m content to leave it as an individual decision. But public health affects everyone, so there has to be room for experts to inform the public about the technical issues, and room for political processes to decide how to use that information in making recommendations and rules. Technical expertise informs technical questions, and the answer to those technical questions inform critical reflection and judgement. Importantly, it’s not just the opinions of vaccine developers which count for the technical questions – there’s an FDA whose purpose is to insure that the vaccines are safe AND effective.\nYann LeCun thinks that builders of AI are the only ones whose opinion really counts. They aren’t afraid of AGI, so the rest of us shouldn’t be either.\nFirst, that’s not true – there are a number of people who work at the cutting edge of AI research and development who are worried about the potential harms of AGI (and at least as importantly, who worry about the actual current harms from irresponsible deployment of AI tools). But I think that’s a red herring anyway, as the relevant expertise here isn’t technical.\nThe only technical insight one needs is that AI systems are developed in a way to maximize their “score” on measurements of how well their outputs reflect the programmers’ goals. If the score measurement perfectly reflects the engineered goals (and those goals are good ones), then there isn’t a problem. But the score measurement never perfectly reflects the engineered goals. I’ve tried to measure student learning in many different ways in my career, but I’ve never managed to find a way to do that that prevented students from “gaming” the system. There always seems to be a way to get a good score without actually learning, and some students always seem to find it.\nMachine learning systems are also quite creative at gaming their scoring systems, allowing them to get good scores without actually doing the thing we wanted them to do. Some examples of this are just funny. Some are pretty disturbing. The point, for those of us who are worried but aren’t cutting edge AI researchers like Yan LeCun, is that engineered products are supposed to satisfy relevant safety guidelines. Pharmaceutical companies have to demonstrate that their products are safe and effective before they can be used. But there isn’t an FDA for algorithms (yet), and if the only opinions that matter are those of the developers, we’re stuck trusting folks who are motivated more by the potential profit than by the relevant ethical questions, and even then those experts don’t really know how safe and effective their products are.\nSo absent any independent assurances, my questions and expertise are just as relevant as LeCun’s. I think that attacking the “information space” with a misinformation bomb is a problem. Deploying automated decision making tools without taking legal responsibility for the outcomes is a problem. Moving fast and breaking the education system with a tool that subverts our entire assessment framework is a problem. There’s of course many more – all of which are going to require some solutions or mitigations or just deciding not to do that.\nIt would be great if the leaders in developing these tools also seemed to be interested in collaborating with the rest of us to figure out which tools might be useful, which need safeguards, and which shouldn’t be used at all. In figuring out what we ought to do, GPT is definitely not all you need."
  },
  {
    "objectID": "blog/posts/expertise/GPT Is Not All You Need.html",
    "href": "blog/posts/expertise/GPT Is Not All You Need.html",
    "title": "GPT Is Not All You Need",
    "section": "",
    "text": "–\nOne of the pillars of my life mission is that humans don’t have to earn the right to think critically. The sneering “You’re not an expert, so you’ll just have to let your betters tell you what to think” crowd has been a consistent frustration for decades. To me, one of the goals of education is freedom from the “tyranny of experts”, not to replace expertise, but to learn enough to be able to ask good questions to inform critical judgements.\nThere’s obviously a tension. Anti-maskers and anti-vaxxers express a similar-sounding view, “I’m just doing my own research, and I don’t think these vaccines are safe,” they say, arguing that they should get to decide for themselves. When decisions truly only affect the individual, I’m content to leave it as an individual decision. But public health affects everyone, so there has to be room for experts to inform the public about the technical issues, and room for political processes to decide how to use that information in making recommendations and rules. Technical expertise informs technical questions, and the answer to those technical questions inform critical reflection and judgement. Importantly, it’s not just the opinions of vaccine developers which count for the technical questions – there’s an FDA whose purpose is to insure that the vaccines are safe AND effective.\nYann LeCun thinks that builders of AI are the only ones whose opinion really counts. They aren’t afraid of AGI, so the rest of us shouldn’t be either.\nFirst, that’s not true – there are a number of people who work at the cutting edge of AI research and development who are worried about the potential harms of AGI (and at least as importantly, who worry about the actual current harms from irresponsible deployment of AI tools). But I think that’s a red herring anyway, as the relevant expertise here isn’t technical.\nThe only technical insight one needs is that AI systems are developed in a way to maximize their “score” on measurements of how well their outputs reflect the programmers’ goals. If the score measurement perfectly reflects the engineered goals (and those goals are good ones), then there isn’t a problem. But the score measurement never perfectly reflects the engineered goals. I’ve tried to measure student learning in many different ways in my career, but I’ve never managed to find a way to do that that prevented students from “gaming” the system. There always seems to be a way to get a good score without actually learning, and some students always seem to find it.\nMachine learning systems are also quite creative at gaming their scoring systems, allowing them to get good scores without actually doing the thing we wanted them to do. Some examples of this are just funny. Some are pretty disturbing. The point, for those of us who are worried but aren’t cutting edge AI researchers like Yan LeCun, is that engineered products are supposed to satisfy relevant safety guidelines. Pharmaceutical companies have to demonstrate that their products are safe and effective before they can be used. But there isn’t an FDA for algorithms (yet), and if the only opinions that matter are those of the developers, we’re stuck trusting folks who are motivated more by the potential profit than by the relevant ethical questions, and even then those experts don’t really know how safe and effective their products are.\nSo absent any independent assurances, my questions and expertise are just as relevant as LeCun’s. I think that attacking the “information space” with a misinformation bomb is a problem. Deploying automated decision making tools without taking legal responsibility for the outcomes is a problem. Moving fast and breaking the education system with a tool that subverts our entire assessment framework is a problem. There’s of course many more – all of which are going to require some solutions or mitigations or just deciding not to do that.\nIt would be great if the leaders in developing these tools also seemed to be interested in collaborating with the rest of us to figure out which tools might be useful, which need safeguards, and which shouldn’t be used at all. In figuring out what we ought to do, GPT is definitely not all you need."
  }
]